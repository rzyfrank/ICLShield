import torch
from .model import ModelHandler
from transformers import AutoModelForCausalLM, AutoTokenizer

if torch.cuda.is_available():
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True
    device = torch.device('cuda')
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device('cpu')

# MODEL_PATH = "meta-llama/Llama-2-7b-hf"
# MODEL_PATH = "meta-llama/Llama-2-7b-chat-hf"
# MODEL_PATH = "meta-llama/Llama-3.1-8B-Instruct"

class LlamaHandler7B(ModelHandler):
    def __init__(self, model_name="meta-llama/Llama-2-7b-chat-hf",
                 max_new_tokens=256):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        self.max_new_tokens = max_new_tokens
        self.eos_token_id = self.tokenizer.eos_token_id
        self.model.eval()
        return

    def response(self, prompt):
        with torch.cuda.amp.autocast(enabled=True):
            with torch.inference_mode():
                input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(device)
                output = self.model.generate(
                    input_ids, eos_token_id=self.eos_token_id, do_sample=True,
                    top_k=50, top_p=0.7, temperature=1.0, max_new_tokens=self.max_new_tokens, 
                    pad_token_id=0, bos_token_id=1,
                )
                output = output[:, len(input_ids[0]) :]
        return self.tokenizer.decode(output[0], skip_special_tokens=True)
    

class LlamaHandler13B(ModelHandler):
    def __init__(self, model_name="meta-llama/Llama-2-13b-chat-hf",
                 max_new_tokens=256):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        self.max_new_tokens = max_new_tokens
        self.eos_token_id = self.tokenizer.eos_token_id
        self.model.eval()
        return

    def response(self, prompt):
        with torch.cuda.amp.autocast(enabled=True):
            with torch.inference_mode():
                input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(device)
                output = self.model.generate(
                    input_ids, eos_token_id=self.eos_token_id, do_sample=True,
                    top_k=50, top_p=0.7, temperature=1.0, max_length=4096, 
                    pad_token_id=0, bos_token_id=1,
                )
                output = output[:, len(input_ids[0]) :]
        return self.tokenizer.decode(output[0], skip_special_tokens=True)
    

class LlamaHandler70B(ModelHandler):
    def __init__(self, model_name="meta-llama/Llama-2-70b-chat-hf",
                 max_new_tokens=256):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        self.max_new_tokens = max_new_tokens
        self.eos_token_id = self.tokenizer.eos_token_id
        self.model.eval()
        # self.model = self.model.to(device)
        return

    def response(self, prompt):
        with torch.cuda.amp.autocast(enabled=True):
            with torch.inference_mode():
                input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(device)
                output = self.model.generate(
                    input_ids, eos_token_id=self.eos_token_id, do_sample=True,
                    top_k=50, top_p=0.7, temperature=1.0, max_length=4096, 
                    pad_token_id=0, bos_token_id=1,
                )
                output = output[:, len(input_ids[0]) :]
        return self.tokenizer.decode(output[0], skip_special_tokens=True)
    



class LlamaHandler3_8B(ModelHandler):
    def __init__(self, model_name="meta-llama/Llama-3.1-8B-Instruct",
                 max_new_tokens=256):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        self.max_new_tokens = max_new_tokens
        self.eos_token_id = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        self.model.eval()
        return

    def response(self, prompt):
        with torch.cuda.amp.autocast(enabled=True):
            with torch.inference_mode():
                input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(device)
                output = self.model.generate(
                    input_ids, eos_token_id=self.eos_token_id, do_sample=True,
                    top_p=0.9, temperature=0.6, max_new_tokens=self.max_new_tokens,
                    pad_token_id=0, bos_token_id=128000, min_new_tokens=1
                )
                output = output[:, len(input_ids[0]) :]
        return self.tokenizer.decode(output[0], skip_special_tokens=True)
    


class LlamaHandler3_70B(ModelHandler):
    def __init__(self, model_name="meta-llama/Llama-3.1-8B-Instruct",
                 max_new_tokens=256):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        self.max_new_tokens = max_new_tokens
        self.eos_token_id = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        self.model.eval()
        return

    def response(self, prompt):
        with torch.cuda.amp.autocast(enabled=True):
            with torch.inference_mode():
                input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(device)
                output = self.model.generate(
                    input_ids, eos_token_id=self.eos_token_id, do_sample=True,
                    top_p=0.9, temperature=0.6, max_new_tokens=self.max_new_tokens,
                    pad_token_id=0, bos_token_id=128000,
                )
                output = output[:, len(input_ids[0]) :]
        return self.tokenizer.decode(output[0], skip_special_tokens=True)